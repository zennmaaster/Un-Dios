package com.castor.feature.messaging.ui

import android.util.Log
import androidx.lifecycle.SavedStateHandle
import androidx.lifecycle.ViewModel
import androidx.lifecycle.viewModelScope
import com.castor.core.common.model.CastorMessage
import com.castor.core.common.model.MessageSource
import com.castor.core.common.util.DateUtils
import com.castor.core.data.repository.MessageRepository
import com.castor.core.inference.InferenceEngine
import dagger.hilt.android.lifecycle.HiltViewModel
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.SharingStarted
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import kotlinx.coroutines.flow.combine
import kotlinx.coroutines.flow.stateIn
import kotlinx.coroutines.launch
import javax.inject.Inject

/**
 * ViewModel for the conversation thread view.
 *
 * Navigation args (extracted from SavedStateHandle or passed directly):
 * - "sender"    -- the conversation contact name
 * - "groupName" -- optional group/channel name
 *
 * Features:
 * - Smart reply suggestions (LLM-backed or rule-based fallback)
 * - Thread summary generation (LLM-backed or manual fallback)
 * - In-conversation message search with filtered results
 * - Reply composition and sending
 */
@HiltViewModel
class ConversationViewModel @Inject constructor(
    private val messageRepository: MessageRepository,
    private val inferenceEngine: InferenceEngine,
    private val savedStateHandle: SavedStateHandle
) : ViewModel() {

    companion object {
        private const val TAG = "ConversationVM"

        /** Default rule-based smart replies when no LLM is available. */
        private val FALLBACK_REPLIES = listOf(
            "Got it!",
            "Thanks!",
            "I'll get back to you",
            "Sounds good!",
            "OK"
        )
    }

    /** Sender resolved from navigation args. */
    val sender: String = savedStateHandle.get<String>("sender") ?: ""

    /** Optional group name from navigation args. */
    val groupName: String? = savedStateHandle.get<String>("groupName")

    /** All messages in this conversation thread, ordered chronologically (ASC). */
    val messages: StateFlow<List<CastorMessage>> =
        messageRepository.getConversationThread(sender, groupName)
            .stateIn(viewModelScope, SharingStarted.WhileSubscribed(5_000), emptyList())

    // ── Smart Reply Suggestions ──────────────────────────────────────────────

    /** 3 contextual reply suggestions, updated when new messages arrive. */
    private val _smartReplies = MutableStateFlow<List<String>>(emptyList())
    val smartReplies: StateFlow<List<String>> = _smartReplies.asStateFlow()

    /** Whether smart replies are currently being generated by the LLM. */
    private val _isGeneratingReplies = MutableStateFlow(false)
    val isGeneratingReplies: StateFlow<Boolean> = _isGeneratingReplies.asStateFlow()

    // ── Reply Input ──────────────────────────────────────────────────────────

    /** Current text in the reply input field. */
    private val _replyText = MutableStateFlow("")
    val replyText: StateFlow<String> = _replyText.asStateFlow()

    // ── Thread Summary ───────────────────────────────────────────────────────

    /** LLM-generated summary of the conversation, or null if not yet generated. */
    private val _threadSummary = MutableStateFlow<String?>(null)
    val threadSummary: StateFlow<String?> = _threadSummary.asStateFlow()

    /** Whether a thread summary is currently being generated. */
    private val _isSummarizing = MutableStateFlow(false)
    val isSummarizing: StateFlow<Boolean> = _isSummarizing.asStateFlow()

    // ── In-Conversation Search ───────────────────────────────────────────────

    /** Current search query within the conversation. */
    private val _conversationSearchQuery = MutableStateFlow("")
    val conversationSearchQuery: StateFlow<String> = _conversationSearchQuery.asStateFlow()

    /** Messages filtered by the current search query. Empty query returns all messages. */
    val filteredMessages: StateFlow<List<CastorMessage>> = combine(
        messages,
        _conversationSearchQuery
    ) { allMessages, query ->
        if (query.isBlank()) {
            allMessages
        } else {
            val q = query.lowercase()
            allMessages.filter { msg ->
                msg.content.lowercase().contains(q) ||
                    msg.sender.lowercase().contains(q)
            }
        }
    }.stateIn(viewModelScope, SharingStarted.WhileSubscribed(5_000), emptyList())

    // ── Initialization ───────────────────────────────────────────────────────

    init {
        // Auto-generate smart replies when the message list changes
        viewModelScope.launch {
            messages.collect { msgs ->
                if (msgs.isNotEmpty()) {
                    generateSmartReplies(msgs)
                }
            }
        }
    }

    // ── Smart Reply Generation ───────────────────────────────────────────────

    /**
     * Generates 3 contextual smart reply suggestions based on recent messages.
     *
     * Uses the on-device InferenceEngine if a model is loaded. Otherwise,
     * falls back to a curated set of rule-based replies.
     */
    private fun generateSmartReplies(currentMessages: List<CastorMessage> = messages.value) {
        viewModelScope.launch {
            _isGeneratingReplies.value = true
            try {
                if (inferenceEngine.isLoaded) {
                    val replies = generateLlmSmartReplies(currentMessages)
                    _smartReplies.value = replies
                } else {
                    _smartReplies.value = generateRuleBasedReplies(currentMessages)
                }
            } catch (e: Exception) {
                Log.e(TAG, "Smart reply generation failed, using fallback", e)
                _smartReplies.value = generateRuleBasedReplies(currentMessages)
            } finally {
                _isGeneratingReplies.value = false
            }
        }
    }

    /**
     * Uses the LLM to generate 3 short, contextual reply suggestions.
     * Sends the last 5 messages as context for the prompt.
     */
    private suspend fun generateLlmSmartReplies(msgs: List<CastorMessage>): List<String> {
        val recentMessages = msgs.takeLast(5)
        val conversationContext = recentMessages.joinToString("\n") { msg ->
            "${msg.sender}: ${msg.content}"
        }

        val prompt = buildString {
            appendLine("Given this conversation, suggest exactly 3 short reply options.")
            appendLine("Each reply should be a brief, natural response (under 8 words).")
            appendLine("Return ONLY the 3 replies, one per line, without numbering or bullets.")
            appendLine()
            appendLine("Conversation:")
            appendLine(conversationContext)
            appendLine()
            appendLine("3 suggested replies:")
        }

        val response = inferenceEngine.generate(
            prompt = prompt,
            systemPrompt = "You are a helpful assistant that suggests brief message replies. Reply with exactly 3 short responses, one per line.",
            maxTokens = 64,
            temperature = 0.8f
        )

        val replies = response
            .lines()
            .map { it.trim() }
            .filter { it.isNotBlank() && it.length <= 50 }
            .map { line ->
                // Strip any leading numbering like "1.", "1)", "- ", etc.
                line.replace(Regex("^[\\d]+[.)]\\s*"), "")
                    .replace(Regex("^[-*]\\s*"), "")
                    .trim()
            }
            .filter { it.isNotBlank() }
            .take(3)

        // If LLM returned fewer than 3 usable replies, pad with fallbacks
        return if (replies.size >= 3) {
            replies
        } else {
            val fallbacks = FALLBACK_REPLIES.shuffled().take(3 - replies.size)
            replies + fallbacks
        }
    }

    /**
     * Generates rule-based smart replies when no LLM model is loaded.
     * Picks 3 contextually appropriate replies based on the last message.
     */
    private fun generateRuleBasedReplies(msgs: List<CastorMessage>): List<String> {
        val lastMessage = msgs.lastOrNull()?.content?.lowercase() ?: ""

        return when {
            lastMessage.contains("?") -> listOf(
                "Yes, sure!",
                "Not right now",
                "Let me check"
            )
            lastMessage.contains("thanks") || lastMessage.contains("thank you") -> listOf(
                "You're welcome!",
                "No problem!",
                "Anytime!"
            )
            lastMessage.contains("meeting") || lastMessage.contains("call") -> listOf(
                "I'll be there",
                "Can we reschedule?",
                "Sounds good!"
            )
            lastMessage.contains("sorry") || lastMessage.contains("apolog") -> listOf(
                "No worries!",
                "It's all good",
                "Don't worry about it"
            )
            lastMessage.contains("help") || lastMessage.contains("issue") -> listOf(
                "I'll look into it",
                "Can you share more details?",
                "On it!"
            )
            else -> FALLBACK_REPLIES.shuffled().take(3)
        }
    }

    // ── Thread Summary ───────────────────────────────────────────────────────

    /**
     * Generates a summary of the conversation thread.
     *
     * Uses the on-device LLM to summarize the last 20 messages. If no model
     * is loaded, produces a manual summary with message count and time range.
     */
    fun summarizeThread() {
        viewModelScope.launch {
            _isSummarizing.value = true
            try {
                val currentMessages = messages.value
                if (currentMessages.isEmpty()) {
                    _threadSummary.value = "No messages to summarize."
                    return@launch
                }

                if (inferenceEngine.isLoaded) {
                    _threadSummary.value = generateLlmSummary(currentMessages)
                } else {
                    _threadSummary.value = generateManualSummary(currentMessages)
                }
            } catch (e: Exception) {
                Log.e(TAG, "Thread summary generation failed", e)
                _threadSummary.value = generateManualSummary(messages.value)
            } finally {
                _isSummarizing.value = false
            }
        }
    }

    /**
     * Sends the last 20 messages to the LLM with a summarization prompt.
     */
    private suspend fun generateLlmSummary(msgs: List<CastorMessage>): String {
        val recentMessages = msgs.takeLast(20)
        val conversationText = recentMessages.joinToString("\n") { msg ->
            "[${DateUtils.formatTime(msg.timestamp)}] ${msg.sender}: ${msg.content}"
        }

        val prompt = buildString {
            appendLine("Summarize this conversation thread in 2-3 concise sentences.")
            appendLine("Focus on the key topics discussed and any action items.")
            appendLine()
            appendLine("Conversation:")
            appendLine(conversationText)
            appendLine()
            appendLine("Summary:")
        }

        val response = inferenceEngine.generate(
            prompt = prompt,
            systemPrompt = "You are a concise conversation summarizer. Provide brief, actionable summaries.",
            maxTokens = 150,
            temperature = 0.3f
        )

        return response.trim()
    }

    /**
     * Generates a manual summary based on message count and time range
     * when no LLM model is available.
     */
    private fun generateManualSummary(msgs: List<CastorMessage>): String {
        val msgCount = msgs.size
        val participants = msgs.map { it.sender }.distinct()
        val firstTimestamp = msgs.minOfOrNull { it.timestamp } ?: 0L
        val lastTimestamp = msgs.maxOfOrNull { it.timestamp } ?: 0L

        return buildString {
            append("Thread: $msgCount message${if (msgCount != 1) "s" else ""}")
            if (participants.size > 1) {
                append(" between ${participants.joinToString(", ")}")
            }
            append(".")
            if (firstTimestamp > 0 && lastTimestamp > 0) {
                append(" Spanning from ${DateUtils.formatTime(firstTimestamp)}")
                append(" to ${DateUtils.formatTime(lastTimestamp)}.")
            }
            append(" Load an on-device model to generate an AI-powered summary.")
        }
    }

    /** Dismisses the thread summary card. */
    fun dismissSummary() {
        _threadSummary.value = null
    }

    // ── In-Conversation Search ───────────────────────────────────────────────

    /** Updates the in-conversation search query. */
    fun setConversationSearchQuery(query: String) {
        _conversationSearchQuery.value = query
    }

    /** Clears the in-conversation search. */
    fun clearConversationSearch() {
        _conversationSearchQuery.value = ""
    }

    // ── Reply Actions ────────────────────────────────────────────────────────

    fun updateReplyText(text: String) {
        _replyText.value = text
    }

    /**
     * Sends a reply message to this conversation.
     *
     * Currently stores the reply locally in the database. The actual cross-app reply
     * mechanism (e.g., replying via notification action) will be connected through
     * the ReplyManager in a future phase.
     */
    fun sendReply() {
        val text = _replyText.value.trim()
        if (text.isEmpty()) return

        viewModelScope.launch {
            // Determine the source from existing messages, or default to WhatsApp
            val source = messages.value.firstOrNull()?.source ?: MessageSource.WHATSAPP

            messageRepository.addMessage(
                source = source,
                sender = "You",
                content = text,
                groupName = groupName,
                notificationKey = null
            )
            _replyText.value = ""
        }
    }

    /**
     * Uses a smart reply chip to populate and send a reply.
     */
    fun sendSmartReply(reply: String) {
        _replyText.value = reply
        sendReply()
    }

    /** Marks all messages in this conversation as read. */
    fun markConversationAsRead() {
        viewModelScope.launch {
            messageRepository.markConversationAsRead(sender, groupName)
        }
    }
}
